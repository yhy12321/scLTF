import scipy.sparse as sp
import numpy as np
import pandas as pd
from sklearn.metrics import adjusted_rand_score, normalized_mutual_info_score
import torch
import torch.nn as nn
from sklearn.preprocessing import StandardScaler
from torch_geometric.data import Data
from collections import defaultdict
from torch.utils.data import DataLoader
from scipy.optimize import linear_sum_assignment


# Import custom functions
from fla_and_cri import generateAnchorsAndAdjacency, Louvain, modularity_contribution
from transformer import TransformerModel, validate_model, evaluate_model, train_model, EarlyStopping, GeneExpressionDataset, predict_full_dataset


if __name__ == '__main__':
    # Unified management of data paths
    # Gene expression matrix before PCA processing
    data_preprocessed_no_pca_path = 'path/to/preprocessed_data_no_pca.csv'
    # Gene expression matrix after PCA processing
    data_pca_path = 'path/to/preprocessed_data_pca.csv'
    # Path to save Louvain clustering results
    louvain_save_path = 'path/to/louvain_clustering_results.csv'
    # Path to save Transformer clustering results
    transformer_save_path = 'path/to/transformer_clustering_results.csv'
    # Path to the ground truth label file
    true_label_path = 'path/to/ground_truth_labels.csv'

    ##############################################################################################################################
    #Preliminary Clustering with the Fast Louvain Algorithm


    # Load the PCA-processed gene expression matrix from a CSV file
    data = pd.read_csv(data_pca_path, index_col=0)
    expression_matrix = data.to_numpy()
    print("Shape of the loaded PCA-processed matrix (rows, columns):", expression_matrix.shape)

    # Generate representative anchors and the sparse similarity matrix between cells and anchors
    Anchors, B = generateAnchorsAndAdjacency(expression_matrix, p=1000, k=10,
                      seed=1,
                      use_multiprocessing=False,
                      num_multiProcesses=4
                      )

    # Append the anchor rows below the original expression matrix
    expression_matrix_with_Anchors = np.vstack((expression_matrix, Anchors))
    print("Matrix shape after adding anchors (rows, columns):", expression_matrix_with_Anchors.shape)
    # Construct the sparse adjacency matrix
    # Compute the total number of rows and columns of the sparse adjacency matrix
    total_size = B.shape[0] + B.shape[1]
    extended_matrix = sp.csr_matrix((total_size, total_size), dtype=B.dtype)
    extended_matrix[:B.shape[0], B.shape[0]:] = B
    extended_matrix[B.shape[0]:, :B.shape[0]] = B.transpose()
    print("Sparse adjacency matrix:")
    print("Number of rows:", extended_matrix.shape[0])
    print("Number of columns:", extended_matrix.shape[1])
    print("Number of anchors:", B.shape[1])


    x = torch.tensor(expression_matrix_with_Anchors, dtype=torch.float)
    adjacency_matrix_coo = extended_matrix.tocoo()
    row = adjacency_matrix_coo.row
    col = adjacency_matrix_coo.col
    edges = np.vstack((row, col))
    weights = adjacency_matrix_coo.data
    weights = (weights - weights.min()) / (weights.max() - weights.min())# Normalize the edge weights
    edge_index = torch.tensor(edges, dtype=torch.long)
    edge_attr = torch.tensor(weights, dtype=torch.float)
    print("Edge index shape:", edge_index.shape)
    print("Edge weight shape:", edge_attr.shape)

    # Perform clustering using the Louvain algorithm
    z = Data(x=x, edge_index=edge_index)
    k, y, partition, G = Louvain(z, edge_attr)
    cluster_df = pd.DataFrame({'cluster_id_L': list(partition.values())})

    loaded_cluster_labels = cluster_df['cluster_id_L'].values.astype(int)
    n = 1000  # Remove the last n rows corresponding to representative anchors from loaded_cluster_labels
    loaded_cluster_labels = loaded_cluster_labels[:-n]
    print("Shape of labels generated by FLA:", loaded_cluster_labels.shape)

    klouvain = pd.DataFrame({'cluster_id_L': loaded_cluster_labels})
    # Save as a CSV file
    klouvain.to_csv(louvain_save_path, index=False)
    print(f"FLA-generated clustering labels have been saved to {louvain_save_path}")

    # Load an independent file containing the ground truth labels
    true_labels_df = pd.read_csv(true_label_path)
    # Check the data type of the labels
    print("Data type of the labels:",
          true_labels_df['cell_type'].dtype)  # 'cell_type' is the column name containing the labels
    # If the labels are already in integer or float format, they can be used directly
    if pd.api.types.is_numeric_dtype(true_labels_df['cell_type']):
        true_labels = true_labels_df['cell_type'].values
        print("Shape of the ground truth labels:", true_labels.shape)
        print("Sample of labels:", true_labels[:10])
    else:# If the labels are in string format, they need to be converted
        from sklearn.preprocessing import LabelEncoder
        label_encoder = LabelEncoder()
        true_labels = label_encoder.fit_transform(true_labels_df['cell_type'])
        print("Shape of the converted labels:", true_labels.shape)
        print("Sample of the converted labels:", true_labels[:10])

    # Compare the clustering results with the ground truth labels
    if len(loaded_cluster_labels) == len(true_labels):
        # Adjusted Rand Index (ARI)
        ari_another = adjusted_rand_score(loaded_cluster_labels, true_labels)
        print("ARI（FLA）：", ari_another)
        # Normalized Mutual Information (NMI)
        nmi_another = normalized_mutual_info_score(loaded_cluster_labels, true_labels)
        print("NMI（FLA）：", nmi_another)
        # Clustering Accuracy (ACC)
        contingency_matrix = np.zeros((max(loaded_cluster_labels) + 1, max(true_labels) + 1), dtype=np.int64)
        for i in range(len(loaded_cluster_labels)):
            contingency_matrix[loaded_cluster_labels[i], true_labels[i]] += 1
        row_ind, col_ind = linear_sum_assignment(-contingency_matrix)
        acc_another = contingency_matrix[row_ind, col_ind].sum() / len(true_labels)
        print("ACC（FLA）：", acc_another)
    else:
        print("Error: loaded_cluster_labels and true_labels have inconsistent lengths.")
    ##############################################################################################################################
    #Representative Cell Selection


    # Compute the modularity contribution of each node (considering edge weights)
    contributions = modularity_contribution(G, partition)
    # Identify the cell with the highest contribution in each cluster, excluding cells with indices beyond the original matrix rows
    num_rows = expression_matrix.shape[0]
    # Save the highest-contributing cell and its corresponding cluster label for each cluster
    cluster_top_cells = {}
    cluster_labels = {}
    # Identify the cell with the highest contribution in each cluster
    cluster_contributions = defaultdict(list)
    for node, comm in partition.items():
        if node < num_rows:  # Ensure that node indices are within the range of the original matrix rows
            cluster_contributions[comm].append((node, contributions[node]))
    for comm, contribs in cluster_contributions.items():
        num_top_nodes = max(1, len(contribs) * 2// 100)  # Select a certain proportion or number
        # num_top_nodes = max(1, 10)
        top_contribs = sorted(contribs, key=lambda x: x[1], reverse=True)[:num_top_nodes]
        cluster_top_cells[comm] = [cell for cell, _ in top_contribs]
        cluster_labels[comm] = comm
    #print("cluster_top_cells:", cluster_top_cells)
    #print("cluster_labels:", cluster_labels)

    # Dynamically select GPU; fall back to CPU if no GPU is available
    print("CUDA available:", torch.cuda.is_available())
    if torch.cuda.is_available():
        print("Number of GPUs:", torch.cuda.device_count())
        for i in range(torch.cuda.device_count()):
            print(f"GPU {i}: {torch.cuda.get_device_name(i)}")
        #gpu_index = torch.cuda.current_device()
        gpu_index = 1
        device = torch.device(f'cuda:{gpu_index}')
    else:
        print("No CUDA devices detected.")
        device = torch.device('cpu')
    print('Using device:', device)

    ############
    # Retrieve the gene expression matrix corresponding to the selected representative cell indices

    chunksize1 = 10000 # Set an appropriate block size
    chunks = pd.read_csv(data_preprocessed_no_pca_path, chunksize=chunksize1)
    data = pd.concat(chunks, axis=0)
    transposed_data = data.transpose()
    gene_expression_data = transposed_data.to_numpy()
    print("Non-PCA matrix shape (rows, columns):", gene_expression_data.shape)
    num_rows = gene_expression_data.shape[0]
    # Extract the required row indices
    selected_indices = []
    for cluster_id, cell_indices in cluster_top_cells.items():
        selected_indices.extend(cell_indices)
    print(f"Length of selected indices: {len(selected_indices)}")
    selected_indices = [idx for idx in selected_indices if idx <= num_rows - 1]
    selected_indices = list(set(selected_indices))
    print(f"Length of indices after selection: {len(selected_indices)}")
    selected_data = gene_expression_data[selected_indices, :]# Extract data based on the row indices
    cells_data = pd.read_csv(true_label_path)
    selected_labels_data = cells_data.iloc[:, 0].values# Extract the specified columns
    cluster_labels1 = cluster_df.iloc[:-n].copy()
    cluster_labels1.insert(0, 'Barcode', selected_labels_data)
    print("Shape of the ground truth labels:", cluster_labels1.shape)
    selected_labels = cluster_labels1.iloc[selected_indices, :]# Extract label data based on row indices

    print("Shape of the extracted gene expression data:", selected_data.shape)
    print("Shape of the extracted labels:", selected_labels.shape)
    ##############################################################################################################################
    #5.Final Clustering with the Transformer Model


    ############
    # Data loading and preprocessing section

    # Dynamically select GPU; fall back to CPU if no GPU is available
    print("CUDA available:", torch.cuda.is_available())
    if torch.cuda.is_available():
        print("Number of GPUs:", torch.cuda.device_count())
        for i in range(torch.cuda.device_count()):
            print(f"GPU {i}: {torch.cuda.get_device_name(i)}")
        #gpu_index = torch.cuda.current_device()
        gpu_index = 1
        device = torch.device(f'cuda:{gpu_index}')
    else:
        print("No CUDA devices detected.")
        device = torch.device('cpu')
    print('Using device:', device)

    # Load the gene expression data and transpose it
    chunksize2 = 10000  # Set an appropriate block size
    chunks = pd.read_csv(data_preprocessed_no_pca_path, index_col=0, chunksize=chunksize2)
    gene_expression_data = pd.concat(chunks, axis=0)
    gene_expression_data = gene_expression_data.transpose()
    gene_expression_data.index = selected_labels_data
    print("Data shape:", gene_expression_data.shape)
    # Retrieve cell data and cluster labels from the selected labels
    cells_data = selected_labels
    train_cell_names = cells_data['Barcode'].values
    y_train_labels = cells_data['cluster_id_L'].values
    unique_clusters_cells_data = cells_data['cluster_id_L'].unique()# Get the unique cluster labels
    print("Number of unique clusters:", len(unique_clusters_cells_data))

    X_train = gene_expression_data.loc[train_cell_names].values# Select training feature data from the gene expression dataset
    # Standardize the data
    scaler = StandardScaler()
    X_train = scaler.fit_transform(X_train)
    #############
    # Model training section

    # Define the number of heads (num_heads) and layers (num_layers) for training
    num_headsss=1
    num_layersss=1
    # Define the input dimension of the model
    input_dim = gene_expression_data.shape[1]
    model = TransformerModel(input_dim=input_dim, num_classes=len(set(y_train_labels)), num_heads=num_headsss, num_layers=num_layersss).to(
        device)
    # Define the loss function and the optimizer
    criterion = nn.CrossEntropyLoss()
    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
    # Create datasets and data loaders for the training and validation sets
    train_dataset = GeneExpressionDataset(X_train, y_train_labels)
    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=4, pin_memory=True)
    # Define the number of training epochs and the number of epochs for early stopping
    num_epochs = 1
    num_epochs_earlystop = 500
    learning_rates = [0.1, 0.01, 0.001, 0.0001]
    batch_sizes = [16, 32, 64, 128]
    results_learning_rates = []
    results_batch_sizes = []

    # Iterate over different learning rates for training and evaluation
    for lr_better in learning_rates:
        model = TransformerModel(input_dim=input_dim, num_classes=len(set(y_train_labels)), num_heads=num_headsss,
                                 num_layers=num_layersss).to(device)
        optimizer = torch.optim.Adam(model.parameters(), lr=lr_better)
        best_val_loss = float('inf')
        best_val_accuracy = 0
        for epoch in range(num_epochs):
            train_model(model, train_loader, optimizer, criterion, num_epochs, device)
            val_loss, val_accuracy = evaluate_model(model, train_loader, criterion, device)
            if val_loss < best_val_loss:
                best_val_loss = val_loss
            if val_accuracy > best_val_accuracy:
                best_val_accuracy = val_accuracy
        results_learning_rates.append(
            {'Learning Rate': lr_better, 'Validation Loss': best_val_loss, 'Validation Accuracy': best_val_accuracy})

    # Iterate over different batch sizes for training and evaluation
    for batch_size in batch_sizes:
        model = TransformerModel(input_dim=input_dim, num_classes=len(set(y_train_labels)), num_heads=num_headsss,
                                 num_layers=num_layersss).to(device)
        optimizer = torch.optim.Adam(model.parameters(), lr=lr_better)
        best_val_loss = float('inf')
        best_val_accuracy = 0
        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)
        for epoch in range(num_epochs):
            train_model(model, train_loader, optimizer, criterion, num_epochs, device)
            val_loss, val_accuracy = evaluate_model(model, train_loader, criterion, device)
            if val_loss < best_val_loss:
                best_val_loss = val_loss
            if val_accuracy > best_val_accuracy:
                best_val_accuracy = val_accuracy
        results_batch_sizes.append(
            {'Batch Size': batch_size, 'Validation Loss': best_val_loss, 'Validation Accuracy': best_val_accuracy})

    # Select the best learning rate and batch size
    best_lr = min(results_learning_rates, key=lambda x: x['Validation Loss'])['Learning Rate']
    best_batch_size = min(results_batch_sizes, key=lambda x: x['Validation Loss'])['Batch Size']
    print(f"Best Learning Rate: {best_lr}")
    print(f"Best Batch Size: {best_batch_size}")

    # Reinitialize the model and optimizer, and add early stopping mechanism
    model = TransformerModel(input_dim=input_dim, num_classes=len(set(y_train_labels)), num_heads=num_headsss,
                             num_layers=num_layersss).to(device)
    optimizer = torch.optim.Adam(model.parameters(), lr=best_lr)
    early_stopping = EarlyStopping(patience=5, delta=1e-2, verbose=True)

    # Create data loaders for the training and validation sets using the best batch size
    train_loader = DataLoader(train_dataset, batch_size=best_batch_size, shuffle=True, num_workers=4, pin_memory=True)

    # Execute the training process and trigger early stopping when necessary
    for epoch in range(num_epochs_earlystop):
        train_model(model, train_loader, optimizer, criterion, num_epochs, device)
        val_loss = validate_model(model, train_loader, criterion, device)
        if early_stopping(val_loss, model):
            print("Early stopping triggered")
            break

    #############
    # Make predictions on the entire dataset and output the results to a file
    scaled_data = scaler.transform(gene_expression_data.values)
    predict_full_dataset(model, torch.tensor(scaled_data, dtype=torch.float32).to(device), gene_expression_data.index,
                         transformer_save_path,device)

    #############
    # Load the predicted clustering results
    predicted_clusters_df = pd.read_csv(transformer_save_path)
    loaded_cluster_labels = predicted_clusters_df['Predicted Cluster'].values.astype(int)
    print("Shape of labels generated by scLTF:", loaded_cluster_labels.shape)

    # Compare the clustering results with the ground truth labels
    if len(loaded_cluster_labels) == len(true_labels):
        # Adjusted Rand Index （ARI）
        ari_another = adjusted_rand_score(loaded_cluster_labels, true_labels)
        print("ARI（scLTF）：", ari_another)
        # Normalized Mutual Information （NMI
        nmi_another = normalized_mutual_info_score(loaded_cluster_labels, true_labels)
        print("NMI（scLTF）：", nmi_another)
        # Clustering Accuracy （ACC）
        contingency_matrix = np.zeros((max(loaded_cluster_labels) + 1, max(true_labels) + 1), dtype=np.int64)
        for i in range(len(loaded_cluster_labels)):
            contingency_matrix[loaded_cluster_labels[i], true_labels[i]] += 1
        row_ind, col_ind = linear_sum_assignment(-contingency_matrix)
        acc_another = contingency_matrix[row_ind, col_ind].sum() / len(true_labels)
        print("ACC（scLTF）：", acc_another)
    else:
        print("Error: loaded_cluster_labels and true_labels have inconsistent lengths.")