import scipy.sparse as sp
import numpy as np
import pandas as pd
from sklearn.metrics import adjusted_rand_score, normalized_mutual_info_score
import torch
from torch_geometric.data import Data
from scipy.optimize import linear_sum_assignment


# Import custom functions
from fla_and_cri import generateAnchorsAndAdjacency, Louvain


if __name__ == '__main__':
    # Unified management of data paths
    # Gene expression matrix before PCA processing
    data_preprocessed_no_pca_path = 'path/to/preprocessed_data_no_pca.csv'
    # Gene expression matrix after PCA processing
    data_pca_path = 'path/to/preprocessed_data_pca.csv'
    # Path to save Louvain clustering results
    louvain_save_path = 'path/to/louvain_clustering_results.csv'
    # Path to save Transformer clustering results
    transformer_save_path = 'path/to/transformer_clustering_results.csv'
    # Path to the ground truth label file
    true_label_path = 'path/to/ground_truth_labels.csv'

    ##############################################################################################################################
    #Preliminary Clustering with the Fast Louvain Algorithm


    # Load the PCA-processed gene expression matrix from a CSV file
    data = pd.read_csv(data_pca_path, index_col=0)
    expression_matrix = data.to_numpy()
    print("Shape of the loaded PCA-processed matrix (rows, columns):", expression_matrix.shape)

    # Generate representative anchors and the sparse similarity matrix between cells and anchors
    Anchors, B = generateAnchorsAndAdjacency(expression_matrix, p=1000, k=10,
                      seed=1,
                      use_multiprocessing=False,
                      num_multiProcesses=4
                      )

    # Append the anchor rows below the original expression matrix
    expression_matrix_with_Anchors = np.vstack((expression_matrix, Anchors))
    print("Matrix shape after adding anchors (rows, columns):", expression_matrix_with_Anchors.shape)
    # Construct the sparse adjacency matrix
    # Compute the total number of rows and columns of the sparse adjacency matrix
    total_size = B.shape[0] + B.shape[1]
    extended_matrix = sp.csr_matrix((total_size, total_size), dtype=B.dtype)
    extended_matrix[:B.shape[0], B.shape[0]:] = B
    extended_matrix[B.shape[0]:, :B.shape[0]] = B.transpose()
    print("Sparse adjacency matrix:")
    print("Number of rows:", extended_matrix.shape[0])
    print("Number of columns:", extended_matrix.shape[1])
    print("Number of anchors:", B.shape[1])


    x = torch.tensor(expression_matrix_with_Anchors, dtype=torch.float)
    adjacency_matrix_coo = extended_matrix.tocoo()
    row = adjacency_matrix_coo.row
    col = adjacency_matrix_coo.col
    edges = np.vstack((row, col))
    weights = adjacency_matrix_coo.data
    weights = (weights - weights.min()) / (weights.max() - weights.min())# Normalize the edge weights
    edge_index = torch.tensor(edges, dtype=torch.long)
    edge_attr = torch.tensor(weights, dtype=torch.float)
    print("Edge index shape:", edge_index.shape)
    print("Edge weight shape:", edge_attr.shape)

    # Perform clustering using the Louvain algorithm
    z = Data(x=x, edge_index=edge_index)
    k, y, partition, G = Louvain(z, edge_attr)
    cluster_df = pd.DataFrame({'cluster_id_L': list(partition.values())})

    loaded_cluster_labels = cluster_df['cluster_id_L'].values.astype(int)
    n = 1000  # Remove the last n rows corresponding to representative anchors from loaded_cluster_labels
    loaded_cluster_labels = loaded_cluster_labels[:-n]
    print("Shape of labels generated by FLA:", loaded_cluster_labels.shape)

    klouvain = pd.DataFrame({'cluster_id_L': loaded_cluster_labels})
    # Save as a CSV file
    klouvain.to_csv(louvain_save_path, index=False)
    print(f"FLA-generated clustering labels have been saved to {louvain_save_path}")

    # Load an independent file containing the ground truth labels
    true_labels_df = pd.read_csv(true_label_path)
    # Check the data type of the labels
    print("Data type of the labels:",
          true_labels_df['cell_type'].dtype)  # 'cell_type' is the column name containing the labels
    # If the labels are already in integer or float format, they can be used directly
    if pd.api.types.is_numeric_dtype(true_labels_df['cell_type']):
        true_labels = true_labels_df['cell_type'].values
        print("Shape of the ground truth labels:", true_labels.shape)
        print("Sample of labels:", true_labels[:10])
    else:# If the labels are in string format, they need to be converted
        from sklearn.preprocessing import LabelEncoder
        label_encoder = LabelEncoder()
        true_labels = label_encoder.fit_transform(true_labels_df['cell_type'])
        print("Shape of the converted labels:", true_labels.shape)
        print("Sample of the converted labels:", true_labels[:10])

    # Compare the clustering results with the ground truth labels
    if len(loaded_cluster_labels) == len(true_labels):
        # Adjusted Rand Index (ARI)
        ari_another = adjusted_rand_score(loaded_cluster_labels, true_labels)
        print("ARI（FLA）：", ari_another)
        # Normalized Mutual Information (NMI)
        nmi_another = normalized_mutual_info_score(loaded_cluster_labels, true_labels)
        print("NMI（FLA）：", nmi_another)
        # Clustering Accuracy (ACC)
        contingency_matrix = np.zeros((max(loaded_cluster_labels) + 1, max(true_labels) + 1), dtype=np.int64)
        for i in range(len(loaded_cluster_labels)):
            contingency_matrix[loaded_cluster_labels[i], true_labels[i]] += 1
        row_ind, col_ind = linear_sum_assignment(-contingency_matrix)
        acc_another = contingency_matrix[row_ind, col_ind].sum() / len(true_labels)
        print("ACC（FLA）：", acc_another)
    else:
        print("Error: loaded_cluster_labels and true_labels have inconsistent lengths.")